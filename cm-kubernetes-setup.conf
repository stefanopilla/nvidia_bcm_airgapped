#######################################################################
##  This config file should be used with cm-kubernetes-setup tool
##  Example:
##      cm-kubernetes-setup -c <filename>
##  
##  Generated by Stefano Pilla (CC) for the Airgapped/disconnected installs
## 	Tested with: BCM10 / RHEL 9.4 / K8s v1.33
## 
##  MD5 checksum of everything after the closing comment: 
##      b5d5b1ae465c0e04dd1e775bef8caaf9
##      to compare:  grep -v '^##' <this_file> | md5sum 
#######################################################################
meta:
  command_line: /cm/local/apps/cm-setup/bin/cm-kubernetes-setup
  date: Fri Jan  9 17:33:13 2026
  generated_with: Kubernetes Setup
  hostname: bcm10-rhel
  package_name: cm-setup-10.0-121078_cm10.0_f61ed990c6
  package_version: 10.0-121078_cm10.0_f61ed990c6
modules:
  kubernetes:
    adv_configuration_bcm_nvidia_packages: false
    airgap:
      helm:
        ca: '/cm/local/apps/containerd/var/etc/certs.d/master.cm.cluster:5000/ca.crt'
        password: ''
        repo: 'oci://master.cm.cluster:5000/helm-charts'
        username: ''
      registry: 'master.cm.cluster:5000'
    apiserver_proxy:
      port: 10443
    ca_certs:
      etcd:
        key: /etc/kubernetes/pki/default/etcd/ca.key
        pem: /etc/kubernetes/pki/default/etcd/ca.crt
    calico:
      excludes:
      - mode_grab: true
        mode_grab_new: true
        path: /etc/cni
      - /opt/cni
      - /run/calico
      - /var/lib/calico
      group: system
      template_file: addons/calico.yaml
    ceph:
      ceph-csi-rbd:
        containers:
          nodeplugin:
            httpMetrics:
              containerPort: 8082
              enabled: true
          provisioner:
            httpMetrics:
              containerPort: 8082
              enabled: true
            replicaCount: 3
      class_name: bright-ceph
      enable: false
      pool:
        min_size: 3
        name: ''
        pg_num: ''
        size: 3
    certs:
      etcdclient:
        key: /etc/kubernetes/pki/default/apiserver-etcd-client.key
        pem: /etc/kubernetes/pki/default/apiserver-etcd-client.crt
      kube:
        trusted_domains:
        - master
        - localhost
    container_runtime: containerd
    container_runtime_packages:
      cm-docker:
      - cm-docker
      containerd:
        dgx:
          redhat:
          - cm-containerd
          ubuntu:
          - cm-containerd
        other:
        - cm-containerd
      cri-o:
      - cri-o
    dashboard_ingress:
      enable: true
      group: system
      template_file: addons/dashboard_ingress.yaml
    default_storage_class: local_path
    dgx:
      network_policies_rules:
        a100: []
        h100: []
    dgx_detect_packages:
      redhat:
      - dgx-release
      ubuntu:
      - dgx-repo
      - dgx-release
    dgx_incompatible_packages:
      containerd:
        dgx:
          redhat:
          - containerd
          - containerd.io
          - nvidia-docker2
          ubuntu:
          - containerd
          - containerd.io
          - nvidia-docker2
    dgx_not_updateable:
      redhat:
      - cuda-driver
      ubuntu: []
    docker.io:
      password: ''
      username: ''
    etcd:
      cluster: kube-default
      configuration_overlay:
        name: kube-default-etcd
      nodes:
      - knode01
      - knode02
      - knode03
      options: []
      packages:
      - cm-etcd
      spool_dir: /var/lib/etcd
    experimental:
      enable_kubeflow_operator: true
      enable_ovn: false
      latest_versions: true
    flannel:
      excludes:
      - mode_grab: true
        mode_grab_new: true
        path: /etc/cni
      - /run/flannel/*
      group: system
      override:
        iface_regex: ''
        ip_blocks: 0
      template_file: addons/flannel.yaml
    grafana_ingress:
      enable: true
      group: system
      template_file: addons/grafana_ingress.yaml
    ingress_controller:
      config:
        cert_file: /root/server.crt
        cert_key_file: /root/server.key
        cert_provided: true
        trusted_domains:
        - fqdn.domain.tld
        - dashboard.fqdn.domain.tld
        - '*.apps.domain.tld'
        - master.cm.cluster
      enable: true
      env:
        CM_KUBE_INGRESS_HTTPS_PORT:
          nodes_env: true
          value: '30445'
        CM_KUBE_INGRESS_HTTP_PORT:
          nodes_env: true
          value: '30082'
        replicas:
          value: '1'
      group: system
      template_file: addons/ingress_controller.yaml
    ingress_proxy:
      enable: false
      port: 443
    kc:
      domain: cluster.local
      edge_site: ''
      external_api_access: true
      external_fqdn: bcm10-rhel.eth.cluster
      helm_version: 3.18.3
      name: default
      network_plugin: calico
      version: '1.33'
    kubeadm:
      imageRepository: ''
    kubernetes_ingress:
      enable: false
      group: system
      template_file: addons/kubernetes_ingress.yaml
    kyverno:
      custom_policies: []
      custom_policies_files: []
      engine:
        chart: kyverno
        enable: true
        helm_opts:
          common:
          - --wait
          - --create-namespace
          name: kyverno
          namespace: kyverno
          values_yaml: "webhooksCleanup:\n  tolerations:\n  -\
            \ key: node-role.kubernetes.io/master\n    effect:\
            \ NoSchedule\n    operator: Exists\n  - key: node-role.kubernetes.io/control-plane\n\
            \    effect: NoSchedule\n    operator: Exists\npolicyReportsCleanup:\n\
            \  tolerations:\n  - key: node-role.kubernetes.io/master\n\
            \    effect: NoSchedule\n    operator: Exists\n  -\
            \ key: node-role.kubernetes.io/control-plane\n   \
            \ effect: NoSchedule\n    operator: Exists\ncleanupJobs:\n\
            \  admissionReports:\n    tolerations:\n    - key:\
            \ node-role.kubernetes.io/master\n      effect: NoSchedule\n\
            \      operator: Exists\n    - key: node-role.kubernetes.io/control-plane\n\
            \      effect: NoSchedule\n      operator: Exists\n\
            \  clusterAdmissionReports:\n    tolerations:\n  \
            \  - key: node-role.kubernetes.io/master\n      effect:\
            \ NoSchedule\n      operator: Exists\n    - key: node-role.kubernetes.io/control-plane\n\
            \      effect: NoSchedule\n      operator: Exists\n\
            admissionController:\n  replicas: 3\n  container:\n\
            \    resources:\n      limits:\n        memory: 3840Mi\n\
            \      requests:\n        cpu: 1000m\n        memory:\
            \ 1280Mi\n  tolerations:\n  - key: node-role.kubernetes.io/master\n\
            \    effect: NoSchedule\n    operator: Exists\n  -\
            \ key: node-role.kubernetes.io/control-plane\n   \
            \ effect: NoSchedule\n    operator: Exists\nbackgroundController:\n\
            \  replicas: 2\n  tolerations:\n  - key: node-role.kubernetes.io/master\n\
            \    effect: NoSchedule\n    operator: Exists\n  -\
            \ key: node-role.kubernetes.io/control-plane\n   \
            \ effect: NoSchedule\n    operator: Exists\ncleanupController:\n\
            \  replicas: 2\n  tolerations:\n  - key: node-role.kubernetes.io/master\n\
            \    effect: NoSchedule\n    operator: Exists\n  -\
            \ key: node-role.kubernetes.io/control-plane\n   \
            \ effect: NoSchedule\n    operator: Exists\nreportsController:\n\
            \  replicas: 2\n  tolerations:\n  - key: node-role.kubernetes.io/master\n\
            \    effect: NoSchedule\n    operator: Exists\n  -\
            \ key: node-role.kubernetes.io/control-plane\n   \
            \ effect: NoSchedule\n    operator: Exists\n"
          version: 3.5.1
      policies:
        chart: kyverno-policies
        enable: true
        helm_opts:
          common:
          - --wait
          name: kyverno-policies
          namespace: kyverno
          version: 3.5.1
        setup_values:
          exclude_namespaces:
            disallow-capabilities:
            - default
            disallow-host-namespaces:
            - default
            disallow-host-path:
            - default
            disallow-host-ports:
            - default
            disallow-privileged-containers:
            - default
          raw_manifest: ''
      repo:
        name: kyverno
        url: https://kyverno.github.io/kyverno/
    landingpage:
      enable: false
      group: system
      template_file: addons/landingpage.yaml
    local_path:
      custom_image: ''
      enable: true
      path: /cm/shared/apps/kubernetes/default/var/volumes
    loki_ingress:
      enable: false
      group: system
      password: ''
      template_file: addons/loki_ingress.yaml
      username: ''
    master:
      configuration_overlay:
        name: kube-default-master
        priority: 510
      labels:
      - node-role.kubernetes.io/master
      - node-role.kubernetes.io/control-plane
      nodes:
      - knode01
      - knode02
      - knode03
      packages:
      - cm-kube-diagnose
      - nginx
      platform_specific_packages:
        redhat:
        - conntrack-tools
        - nginx-all-modules
        - python3-netifaces
        suse:
        - conntrack-tools
        - python3-netifaces
        ubuntu:
        - conntrack
        - python3-netifaces
        ubuntu-2404:
        - libnginx-mod-stream
      shared_packages:
      - cm-kubernetes-permissions-manager
      - cm-kubernetes-local-path-provisioner
    metallb:
      bfd_profiles: {}
      bgp_advertisements: {}
      bgp_peers: {}
      ip_address_pools: {}
      l2_advertisements: {}
    modulefiles:
      etcd:
        template: templates/etcd.module
      kube:
        template: templates/kube.module
    networks:
      internal:
        name: internalnet
      pod:
        base: 172.29.0.0
        bits: 16
        name: kube-default-pod
      service:
        base: 10.150.0.0
        bits: 16
        name: kube-default-service
    nvaie:
      api_key: ''
      registry_email: ''
    nvidia:
      enable: false
      install_packages: true
      install_toolkit_package: true
    nvidia_packages:
      dgx: []
      other:
      - cuda-dcgm
      - cuda-driver
      - cuda-fabric-manager
    nvidia_toolkit_packages:
      dgx:
      - nvidia-container-toolkit
      other:
      - cm-nvidia-container-toolkit
    operators:
      Ceph CSI RBD:
        helm_opts:
          chart: ceph-csi-rbd
          common:
          - --create-namespace
          - --wait
          name: ceph-csi-rbd
          namespace: ceph-csi-rbd
          version: 3.15.0
        install_helm: false
        kind: storage indirect enable
        repo:
          name: ceph-csi
          url: https://ceph.github.io/csi-charts
      Grafana Alloy:
        config:
          kube_events: true
          pod_logs: true
          varlog: false
        conflicts:
          operators:
          - Grafana Promtail
        dependencies:
          operators:
          - Grafana Loki
        helm_opts:
          chart: alloy
          common:
          - --wait
          - --create-namespace
          name: alloy
          namespace: alloy
          values_yaml: "alloy:\n  configMap:  # configmap should\
            \ be created before operator install\n    create:\
            \ false\n    name: alloy-config\n    key: config.alloy\n\
            \  mounts:\n    varlog: true  # allow to collect node\
            \ logs\ncontroller:\n  tolerations:\n    # schedule\
            \ also on the controller\n    - key: node-role.kubernetes.io/master\n\
            \      operator: Exists\n      effect: NoSchedule\n\
            \    - key: node-role.kubernetes.io/control-plane\n\
            \      operator: Exists\n      effect: NoSchedule\n"
          version: 1.2.1
          versions:
          - 1.2.1
          - latest
        install_helm: false
        kind: regular
        repo:
          name: grafana
          url: https://grafana.github.io/helm-charts
      Grafana Loki:
        dependencies:
          addons:
          - Ingress Controller (Nginx)
        helm_opts:
          chart: loki
          common:
          - --wait
          - --create-namespace
          name: loki
          namespace: loki
          values_yaml: "# Note: When running replicas of a single\
            \ binary, you must configure object storage.\nloki:\n\
            \  commonConfig:\n    replication_factor: 1\n  auth_enabled:\
            \ false\n  storage:\n    type: 's3'  # filesystem\
            \ = there's no real persistence here.\n  # `schemaConfig`\
            \ is template in the original values.yaml\n  schemaConfig:\n\
            \    configs:\n      - from: \"2024-04-01\"\n    \
            \    store: tsdb\n        object_store: s3  # filesystem\
            \ = there's no real persistence here.\n        schema:\
            \ v13\n        index:\n          prefix: loki_index_\n\
            \          period: 24h\ndeploymentMode: SingleBinary\n\
            singleBinary:\n  replicas: 1\n\n# minio allow to switch\
            \ storage to `s3` and use > 1 replicas\n# Minio requires\
            \ 2 to 16 drives for erasure code (drivesPerNode *\
            \ replicas)\n# https://docs.min.io/docs/minio-erasure-code-quickstart-guide\n\
            minio:\n  enabled: true\n\n# Different between SingleBinary\
            \ and Distributed\ningester:\n  replicas: 0\nquerier:\n\
            \  replicas: 0\n  maxUnavailable: 0\nqueryFrontend:\n\
            \  replicas: 0\n  maxUnavailable: 0\nqueryScheduler:\n\
            \  replicas: 0\ndistributor:\n  replicas: 0\n  maxUnavailable:\
            \ 0\ncompactor:\n  replicas: 0\nindexGateway:\n  replicas:\
            \ 0\n  maxUnavailable: 0\n\nbackend:\n  replicas:\
            \ 0\nread:\n  replicas: 0\nwrite:\n  replicas: 0\n"
          version: 6.40.0
          versions:
          - 6.40.0
          - latest
        install_helm: false
        kind: regular
        repo:
          name: grafana
          url: https://grafana.github.io/helm-charts
      Grafana Promtail:
        config:
          varlog: false
        conflicts:
          operators:
          - Grafana Alloy
        dependencies:
          operators:
          - Grafana Loki
        deprecated: true
        helm_opts:
          chart: promtail
          common:
          - --wait
          - --create-namespace
          name: promtail
          namespace: promtail
          values_yaml: "config:\n  clients:\n    - url: http://loki-gateway.loki.svc/loki/api/v1/push\
            \  # publish data to loki\n      tenant_id: 1\n\n\
            \  snippets:\n    extraScrapeConfigs: \"\"\n\nextraVolumes:\n\
            \  - name: varlogs\n    hostPath:\n      path: /var/log\n\
            extraVolumeMounts:\n  - name: varlogs\n    mountPath:\
            \ /var/host_logs\n    readOnly: true\n\nextraArgs:\
            \ [\"-config.expand-env=true\"]\n"
          version: 6.17.0
          versions:
          - 6.17.0
          - latest
        install_helm: false
        kind: regular
        repo:
          name: grafana
          url: https://grafana.github.io/helm-charts
      Jupyter Kernel Operator:
        api_available: false
        apiresource: cmjupyterkernels.apps.brightcomputing.com
        helm_opts:
          name: cm-jupyter-kernel-operator
        install_helm: true
        kind: BCM
        package: cm-jupyter-kernel-operator
        path: /cm/shared/apps/jupyter-kernel-operator
        permission_apiresource: CmKubernetesOperatorPermissionsJupyterKernel
      Kata:
        helm_opts:
          chart: kata-deploy
          common:
          - --create-namespace
          name: kata-deploy
          namespace: kata-deploy
          values_yaml: "kataDeploy:\n  imagePullPolicy: Always\n\
            \  imagePullSecrets: []\n  repository: \"quay.io/redhat_z92c6/kata-deploy\"\
            \n  # version can be latest or development\n  version:\
            \ \"bcm\"\n  # k8s-dist can be k8s, k3s, rke2, k0s\n\
            \  k8sDistribution: \"BCM\"\n  debug: \"false\"\n\
            \  shims: \"\"\n  defaultShim: \"\"\n  createRuntimeClasses:\
            \ \"true\"\n  createDefaultRuntimeClass: \"false\"\
            \n  allowedHypervisorAnnotations: \"\"\n  containerdDropInConf:\
            \ \"zz-kata-deploy.toml\"\n  installPrefix: \"/opt/kata/bin\"\
            \n"
        install_helm: false
        kind: regular indirect enable
        repo:
          name: kata-deploy
          url: oci://quay.io/redhat_z92c6/helm/kata-deploy
      Knative Operator:
        config:
          eventing:
            deploy: false
            namespace: knative-eventing
            sources: []
          serving:
            deploy: false
            namespace: knative-serving
        helm_opts:
          chart: knative-operator
          common:
          - --wait
          - --create-namespace
          name: knative-operator
          namespace: knative-operator
          version: v1.19.2
        install_helm: false
        kind: regular
        repo:
          name: knative-operator
          url: https://knative.github.io/operator
      Kubernetes Dashboard:
        helm_opts:
          chart: kubernetes-dashboard
          common:
          - --create-namespace
          - --wait
          name: kubernetes-dashboard
          namespace: kubernetes-dashboard
          values_yaml: "app:\n  tolerations:\n  - key: node-role.kubernetes.io/master\n\
            \    effect: NoSchedule\n  - key: node-role.kubernetes.io/control-plane\n\
            \    effect: NoSchedule\n"
          version: 7.13.0
        install_helm: true
        kind: regular
        repo:
          name: kubernetes-dashboard
          url: https://kubernetes.github.io/dashboard/
      Kubernetes MPI Operator:
        api_available: false
        apiresource: mpijobs.kubeflow.org
        helm_opts:
          name: cm-kubernetes-mpi-operator
        install_helm: true
        kind: BCM
        package: cm-kubernetes-mpi-operator
        path: /cm/shared/apps/kubernetes-mpi-operator
        permission_apiresource: CmKubernetesOperatorPermissionsMpi
      Kubernetes Metrics Server:
        helm_opts:
          chart: metrics-server
          common:
          - --wait
          name: metrics-server
          namespace: kube-system
          values_yaml: "replicas: 2  # bumped this for cm-scale\n\
            containerPort: 4443\ndefaultArgs:\n- --cert-dir=/tmp\n\
            - --kubelet-preferred-address-types=Hostname,InternalIP,ExternalIP\
            \  # patched by Bright\n- --kubelet-use-node-status-port\n\
            - --metric-resolution=15s\ntolerations:\n- effect:\
            \ NoSchedule\n  operator: \"Exists\"\n  key: \"node-role.kubernetes.io/master\"\
            \n- effect: NoSchedule\n  operator: \"Exists\"\n \
            \ key: \"node-role.kubernetes.io/control-plane\"\n"
          version: 3.13.0
        install_helm: true
        kind: regular
        repo:
          name: metrics-server
          url: https://kubernetes-sigs.github.io/metrics-server/
      Kubernetes State Metrics:
        helm_opts:
          chart: kube-state-metrics
          common:
          - --wait
          name: kube-state-metrics
          namespace: kube-system
          values_yaml: "tolerations:\n- effect: NoSchedule\n \
            \ operator: \"Exists\"\n  key: \"node-role.kubernetes.io/master\"\
            \n- effect: NoSchedule\n  operator: \"Exists\"\n \
            \ key: \"node-role.kubernetes.io/control-plane\"\n"
          version: 6.3.0
        install_helm: true
        kind: regular
        repo:
          name: prometheus-community
          url: https://prometheus-community.github.io/helm-charts
      LeaderWorkerSet operator:
        helm_opts:
          chart: lws
          common:
          - --wait
          - --create-namespace
          name: lws
          namespace: lws-system
          version: v0.7.0
        install_helm: false
        kind: regular
        repo:
          name: lws
          url: oci://registry.k8s.io/lws/charts/lws
      MetalLB:
        helm_opts:
          chart: metallb
          common:
          - --create-namespace
          - --wait
          name: metallb
          namespace: metallb-system
          version: 0.15.2
          versions:
          - 0.15.2
          - latest
        install_helm: true
        kind: regular
        repo:
          name: metallb
          url: https://metallb.github.io/metallb
      NIM Operator:
        after:
        - NVIDIA GPU Operator
        - LeaderWorkerSet operator
        api_available: false
        apiresource: nimservices.apps.nvidia.com
        dependencies:
          operators:
          - NVIDIA GPU Operator
          - LeaderWorkerSet operator
        helm_opts:
          chart: k8s-nim-operator
          common:
          - --wait
          - --create-namespace
          name: k8s-nim-operator
          namespace: nim-operator
          version: 2.0.2
        install_helm: false
        kind: regular
        permission_apiresource: CmKubernetesOperatorPermissionsNim
        repo:
          name: nvidia
          url: https://helm.ngc.nvidia.com/nvidia
      NVIDIA GPU Operator:
        helm_opts:
          chart: gpu-operator
          common:
          - --wait
          - -n gpu-operator
          - --create-namespace
          custom_yaml_path: ''
          name: gpu-operator
          values_yaml: "cdi:\n  default: false\n  enabled: true\n\
            dcgm:\n  enabled: false\ndcgmExporter:\n  enabled:\
            \ true\n  serviceMonitor:\n    enabled: false\ndevicePlugin:\n\
            \  enabled: true\n  env:\n  - name: DEVICE_LIST_STRATEGY\n\
            \    value: volume-mounts\ndriver:\n  enabled: false\n\
            \  rdma:\n    enabled: false\nkataManager:\n  enabled:\
            \ false\n  env:\n  - name: CONTAINERD_CONFIG\n   \
            \ value: /cm/local/apps/containerd/var/etc/conf.d/yy-kata-containers.toml\n\
            mig:\n  strategy: single\nmigManager:\n  enabled:\
            \ true\nnfd:\n  enabled: true\nsandboxWorkloads:\n\
            \  enabled: false\ntoolkit:\n  enabled: false\n  env:\n\
            \  - name: CONTAINERD_CONFIG\n    value: /cm/local/apps/containerd/var/etc/conf.d/nvidia-cri.toml\n\
            \  - name: ACCEPT_NVIDIA_VISIBLE_DEVICES_ENVVAR_WHEN_UNPRIVILEGED\n\
            \    value: 'false'\n  - name: ACCEPT_NVIDIA_VISIBLE_DEVICES_AS_VOLUME_MOUNTS\n\
            \    value: 'true'\nvalidator:\n  driver:\n    env:\n\
            \    - name: DISABLE_DEV_CHAR_SYMLINK_CREATION\n \
            \     value: 'true'\n"
          version: v25.3.3
          versions:
          - v25.3.2
          - v25.3.3
          - latest
        install_helm: true
        kind: regular
        repo:
          name: nvidia
          url: https://helm.ngc.nvidia.com/nvidia
        require_no_api_key: true
      NetQ:
        config:
          nodes: []
          package_agent: ''
          package_apps: ''
          tarball: ''
          virtual_ip: ''
        configuration_overlay:
          name: kube-default-netq
          priority: 501
        conflicts:
          addons:
          - Ingress Controller (Nginx)
          operators: []
        install_helm: false
        kind: regular
      Network Operator:
        after:
        - NVIDIA GPU Operator
        config:
          gpu_model: ''
          networks: []
        helm_opts:
          chart: network-operator
          common:
          - --wait
          - --create-namespace
          - -n network-operator
          custom_yaml_path: ''
          name: network-operator
          namespace: network-operator
          values_yaml: "nfd:\n  enabled: true\nnicConfigurationOperator:\n\
            \  enabled: false\nsriovNetworkOperator:\n  enabled:\
            \ true\n"
          version: 25.7.0
          versions:
          - 25.7.0
          - 25.4.0
          - latest
        install_helm: true
        kind: regular
        repo:
          name: nvidia
          url: https://helm.ngc.nvidia.com/nvidia
        require_no_api_key: true
      OVN CNI:
        config:
          customizations:
            values-multi-node.yaml:
            - tags.ovn-db-standalone=true
            - tags.ovn-db-raft=false
            - k8sAPIServer=${APISERVER_URL}
            - serviceNetwork=${SERVICE_NETWORK}
            - global.dockerConfigSecret.auth="${BASE64_OAUTH_TOKEN}"
            - global.dockerConfigSecret.create=true
            - ovn-north.replicaCount=1
            - ovnkube-master.replicaCount=1
        helm_opts:
          chart: ovn-kubernetes
          common:
          - -f ${PATH}/ovn-kubernetes/values-multi-node.yaml
          - -f ${PATH}/ovn-kubernetes/values-images.yaml
          - --wait
          name: ovn-k8s
          version: 1.1.13
        install_helm: false
        kind: OVN_CNI
        repo:
          credentials:
            oauth_token: ''
          name: nvcr-sdn
          url: https://helm.ngc.nvidia.com/nv-ngn/sdn
      PostgreSQL Operator:
        api_available: false
        apiresource: postgresqls.acid.zalan.do
        helm_opts:
          chart: postgres-operator
          common:
          - --create-namespace
          - --wait
          name: postgres-operator
          namespace: postgres-operator
          values_yaml: "image:\n  registry: ghcr.io  # multiarch\
            \ images\n  repository: zalando/postgres-operator\n"
          version: 1.14.0
          versions:
          - 1.12.2
          - 1.13.0
          - 1.14.0
          - latest
        install_helm: false
        kind: regular
        permission_apiresource: CmKubernetesOperatorPermissionsPostgresql
        repo:
          name: postgres-operator-charts
          url: https://opensource.zalando.com/postgres-operator/charts/postgres-operator
      Prometheus Adapter:
        helm_opts:
          chart: prometheus-adapter
          common:
          - --wait
          - --create-namespace
          name: prometheus-adapter
          namespace: prometheus
          values_yaml: "rbac:\n  create: true  # default\nprometheus:\n\
            \  url: \"http://prometheus-operated.prometheus.svc\"\
            \n  port: 9090  # default\n"
          version: 5.1.0
        install_helm: true
        kind: regular
        repo:
          name: prometheus-community
          url: https://prometheus-community.github.io/helm-charts
      Prometheus Operator Stack:
        after:
        - Grafana Loki
        helm_opts:
          chart: kube-prometheus-stack
          common:
          - --wait
          - -n prometheus
          - --create-namespace
          name: kube-prometheus-stack
          values_yaml: "grafana:\n  additionalDataSources: []\n\
            \  grafana.ini:\n    server:\n      root_url: '%(protocol)s://%(domain)s:%(http_port)s/grafana/'\n\
            \      serve_from_sub_path: true\n  persistence:\n\
            \    enabled: true\n    storageClassName: local-path\n\
            \  sidecar:\n    dashboards:\n      enabled: true\n\
            \      label: grafana_dashboard\n      labelValue:\
            \ '1'\n      searchNamespace: ALL\nprometheus:\n \
            \ prometheusSpec:\n    additionalScrapeConfigs:\n\
            \    - job_name: gpu-metrics\n      kubernetes_sd_configs:\n\
            \      - namespaces:\n          names:\n         \
            \ - gpu-operator\n        role: endpoints\n      metrics_path:\
            \ /metrics\n      relabel_configs:\n      - action:\
            \ drop\n        regex: .*-node-feature-discovery-master\n\
            \        source_labels:\n        - __meta_kubernetes_endpoints_name\n\
            \      - action: replace\n        source_labels:\n\
            \        - __meta_kubernetes_pod_node_name\n     \
            \   target_label: kubernetes_node\n      scheme: http\n\
            \      scrape_interval: 1s\nprometheus-node-exporter:\n\
            \  prometheus:\n    monitor:\n      attachMetadata:\n\
            \        node: true\n      relabelings:\n      - action:\
            \ replace\n        sourceLabels:\n        - __meta_kubernetes_pod_node_name\n\
            \        targetLabel: kubernetes_node\n"
          version: 77.6.2
        install_helm: true
        kind: regular
        repo:
          name: prometheus-community
          url: https://prometheus-community.github.io/helm-charts
      Run:ai (SaaS):
        config:
          oidc:
            client_id: runai
            cors_allowed_origins: https://name.run.ai
            issuer_url: https://app.run.ai/auth/realms/name
            username_prefix: '-'
          tenant: name
        dependencies:
          addons:
          - Ingress Controller (Nginx)
          operators:
          - Prometheus Operator Stack
          - Prometheus Adapter
          - NVIDIA GPU Operator
          - LeaderWorkerSet operator
          - Knative Operator
        helm_opts:
          chart: cluster-installer
          common:
          - --create-namespace
          - --wait
          name: cluster-installer
          namespace: runai
          version: 2.22.15
          versions:
          - 2.22.15
          - 2.20.25
          - 2.19.45
          - 2.19.25
          - 2.18.64
          - 2.18.45
        install_helm: false
        kind: regular
        repo:
          credentials:
            required: true
          name: runai
          url: https://runai.jfrog.io/artifactory/run-ai-charts
      Run:ai (self-hosted):
        config:
          ca_cert: ''
          deploy_cluster: false
          domain_name: ${EXTERNAL_FQDN}
          ingress_cert_file: ''
          ingress_key_file: ''
          jfrog_credentials: ''
        dependencies:
          addons:
          - Ingress Controller (Nginx)
          operators:
          - Prometheus Operator Stack
          - Prometheus Adapter
          - NVIDIA GPU Operator
        helm_opts:
          chart: control-plane
          common:
          - --create-namespace
          - --wait
          name: runai-backend
          namespace: runai-backend
          timeout: 20m0s
          values_yaml: "global:\n  customCA:\n    enabled: true\n\
            \  domain: ${EXTERNAL_FQDN}\n  affinity:\n    nodeAffinity:\n\
            \      preferredDuringSchedulingIgnoredDuringExecution:\n\
            \        - weight: 1\n          preference:\n    \
            \        matchExpressions:\n              - key: node-role.kubernetes.io/runai-system\n\
            \                operator: Exists\nassetsService:\n\
            \  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \nauditService:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \nauthorization:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \nbackend:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \ncliExposer:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \nclusterService:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \ndatavolumes:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \nemailService:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \nfrontend:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \nidentityManager:\n  tolerations:\n    - key: \"\
            node-role.kubernetes.io/control-plane\"\n      operator:\
            \ \"Exists\"\n      effect: \"NoSchedule\"\nk8sObjectsTracker:\n\
            \  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \nmetricsService:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \norgUnitService:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \npolicyService:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \npresetsLoader:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \nredoc:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \ntenantsManager:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \ntraefik:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \ntrialService:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \nworkloads:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \n# dependencies\ngrafana:\n  tolerations:\n    -\
            \ key: \"node-role.kubernetes.io/control-plane\"\n\
            \      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \nkeycloakx:\n  tolerations:\n    - key: \"node-role.kubernetes.io/control-plane\"\
            \n      operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \nnotificationsService:\n  tolerations:\n    - key:\
            \ \"node-role.kubernetes.io/control-plane\"\n    \
            \  operator: \"Exists\"\n      effect: \"NoSchedule\"\
            \npostgresql:\n  primary:\n    tolerations:\n    \
            \  - key: \"node-role.kubernetes.io/control-plane\"\
            \n        operator: \"Exists\"\n        effect: \"\
            NoSchedule\"\nredisQueue:\n  master:\n    nodeAffinityPreset:\n\
            \      type: soft\n      key: \"node-role.kubernetes.io/runai-system\"\
            \n      values:\n        - \"true\"\n    tolerations:\n\
            \      - key: \"node-role.kubernetes.io/control-plane\"\
            \n        operator: \"Exists\"\n        effect: \"\
            NoSchedule\"\nthanos:\n  receive:\n    tolerations:\n\
            \      - key: \"node-role.kubernetes.io/control-plane\"\
            \n        operator: \"Exists\"\n        effect: \"\
            NoSchedule\"\n  query:\n    tolerations:\n      -\
            \ key: \"node-role.kubernetes.io/control-plane\"\n\
            \        operator: \"Exists\"\n        effect: \"\
            NoSchedule\"\n"
          version: 2.22.x
          versions:
          - 2.22.x
          - 2.21.x
          - 2.20.x
          - 2.19.x
          - 2.18.x
          - 2.17.x
          - custom
        install_helm: false
        kind: regular
        repo:
          credentials:
            required: true
          name: runai-backend
          url: https://runai.jfrog.io/artifactory/cp-charts-prod
      SDN dashboard:
        dependencies:
          operators:
          - Prometheus Operator Stack
        helm_opts:
          chart: sdn-dashboard
          common:
          - --create-namespace
          - --wait
          - --set global.namespace=prometheus
          name: sdn-dashboard
          namespace: prometheus
          version: 1.0.3
        install_helm: false
        kind: regular
        prio: 10
        repo:
          credentials:
            required: true
          name: nvcr-sdn
          url: https://helm.ngc.nvidia.com/nv-ngn/sdn
      Spark Operator:
        api_available: false
        apiresource: sparkapplications.sparkoperator.k8s.io
        helm_opts:
          chart: spark-operator
          common:
          - --wait
          - --create-namespace
          name: spark-operator
          namespace: spark-operator
          values_yaml: "webhook:\n  enable: true\nspark:\n  jobNamespaces:\
            \ []\n"
          version: 2.3.0
        install_helm: false
        kind: regular
        permission_apiresource: CmKubernetesOperatorPermissionsSpark
        repo:
          name: spark-operator
          url: https://kubeflow.github.io/spark-operator
    ovn:
      artifactory_cert: ''
      artifactory_url: edge.urm.nvidia.com/artifactory/sw-ngn-sdn-rpm
      artifactory_username: ''
      excludes: []
      group: system
      ovs_path: /ovs/nvp-ovs-release/8/x86_64/
      packages:
        redhat:
        - epel-release
        - jemalloc-devel
        - openvswitch-${VERSION}
        - openvswitch-devel-${VERSION}
        ubuntu:
        - libjemalloc2
        - openvswitch-switch=${VERSION}
        - openvswitch-common=${VERSION}
        - python3-openvswitch=${VERSION}
      sdn_path: /sdn/nm/8/x86_64/
      skip_prerequisites_checks: false
      username: ''
      version: 2.17.1.8825f15b.15843596
    permissions_manager:
      custom_controller_image: ''
      custom_rbac_proxy_image: ''
      enable: true
    registry_mirror: ''
    roles:
      containerd:
        excludes:
        - mode_grab: true
          mode_grab_new: true
          path: /var/lib/containerd
        kind: generic
        sandbox_image: ''
        services:
        - containerd
      crio:
        excludes:
        - /var/lib/containers
        kind: generic
        services:
        - crio
      netq:
        excludes:
        - mode_grab: true
          mode_grab_new: true
          path: /etc/logrotate.d/kubelet
        - mode_grab: true
          mode_grab_new: true
          path: /etc/logrotate.d/docker
        - mode_grab: true
          mode_grab_new: true
          path: /etc/logrotate.d/container
        - mode_grab: true
          mode_grab_new: true
          path: /mnt
        - mode_grab: true
          mode_grab_new: true
          path: /mnt/*
        - mode_grab: true
          mode_grab_new: true
          path: /data
        - mode_grab: true
          mode_grab_new: true
          path: /data/*
        - mode_grab: true
          mode_grab_new: true
          path: /etc/python3.7
        - mode_grab: true
          mode_grab_new: true
          path: /etc/python3.7/*
        - mode_grab: true
          mode_grab_new: true
          path: /etc/python-netq-apps
        - mode_grab: true
          mode_grab_new: true
          path: /etc/python-netq-apps/*
        - mode_grab: true
          mode_grab_new: true
          path: /etc/netq-telemetry-dump
        - mode_grab: true
          mode_grab_new: true
          path: /etc/motd
        - mode_grab: true
          mode_grab_new: true
          path: /etc/crictl.yaml
        - mode_grab: true
          mode_grab_new: true
          path: /etc/app-release
        - mode_grab: true
          mode_grab_new: true
          path: /etc/netq
        - mode_grab: true
          mode_grab_new: true
          path: /etc/netq/*
        - mode_grab: true
          mode_grab_new: true
          path: /etc/rsyslog.d/35-container.conf
        - mode_grab: true
          mode_grab_new: true
          path: /etc/rsyslog.d/30-kubelet.conf
        - mode_grab: true
          mode_grab_new: true
          path: /etc/sudoers.d/netq-admin
        - mode_grab: true
          mode_grab_new: true
          path: /etc/systemd/journald.conf.d/netq-journal.conf
        - mode_grab: true
          mode_grab_new: true
          path: /root/.cache/motd.legal-displayed
        - mode_grab: true
          mode_grab_new: true
          path: /usr/bin/python3.7m
        - mode_grab: true
          mode_grab_new: true
          path: /usr/bin/python3.7
        - mode_grab: true
          mode_grab_new: true
          path: /usr/lib/python3.7
        - mode_grab: true
          mode_grab_new: true
          path: /usr/lib/python3.7/*
        - mode_grab: true
          mode_grab_new: true
          path: /usr/local/lib/python3.7
        - mode_grab: true
          mode_grab_new: true
          path: /usr/local/lib/python3.7/*
        - mode_grab: true
          mode_grab_new: true
          path: /usr/sbin/vm-backuprestore.sh
        - mode_grab: true
          mode_grab_new: true
          path: /usr/sbin/netqdump.sh
        - mode_grab: true
          mode_grab_new: true
          path: /usr/sbin/createbackup.sh
        - mode_grab: true
          mode_grab_new: true
          path: /usr/sbin/backuprestore.sh
        - mode_grab: true
          mode_grab_new: true
          path: /usr/share/binfmts/python3.7
        - mode_grab: true
          mode_grab_new: true
          path: /usr/share/doc/python3.7-minimal
        - mode_grab: true
          mode_grab_new: true
          path: /usr/share/doc/python3.7-minimal/*
        - mode_grab: true
          mode_grab_new: true
          path: /usr/share/doc/libpython3.7-minimal
        - mode_grab: true
          mode_grab_new: true
          path: /usr/share/doc/libpython3.7-minimal/*
        - mode_grab: true
          mode_grab_new: true
          path: /usr/share/doc/libpython3.7-stdlib
        - mode_grab: true
          mode_grab_new: true
          path: /usr/share/lintian/overrides/python3.7-minimal
        - mode_grab: true
          mode_grab_new: true
          path: /usr/share/lintian/overrides/libpython3.7-stdlib
        - mode_grab: true
          mode_grab_new: true
          path: /usr/share/lintian/overrides/libpython3.7-minimal
        - mode_grab: true
          mode_grab_new: true
          path: /usr/share/man/man1/python3.7m.1.gz
        - mode_grab: true
          mode_grab_new: true
          path: /usr/share/man/man1/python3.7.1.gz
        - mode_grab: true
          mode_grab_new: true
          path: /var/lib/logrotate/status
        - mode_grab: true
          mode_grab_new: true
          path: /var/lib/postfix/smtp_scache.db
        - mode_grab: true
          mode_grab_new: true
          path: /var/lib/postfix/prng_exch
        - mode_grab: true
          mode_grab_new: true
          path: /var/lib/python/python3.7_installed
        - mode_grab: true
          mode_grab_new: true
          path: /cm/local/apps/containerd/var/etc/certs.d/docker-registry:5000/hosts.toml
        kind: generic
        services:
        - netqd
      ovn:
        excludes:
        - mode_grab: true
          mode_grab_new: true
          path: /etc/openvswitch
        - mode_grab: true
          mode_grab_new: true
          path: /etc/openvswitch/*
        - mode_grab: true
          mode_grab_new: true
          path: /etc/systemd/system/multi-user.target.wants/openvswitch.service
        - mode_grab: true
          mode_grab_new: true
          path: /usr/local/bin/set-ovs-config.sh
        - mode_grab: true
          mode_grab_new: true
          path: /etc/systemd/system/ovs-config.service
        - mode_grab: true
          mode_grab_new: true
          path: /etc/systemd/system/multi-user.target.wants/ovs-config.service
        - mode_grab: true
          mode_grab_new: true
          path: /etc/ovn
        - mode_grab: true
          mode_grab_new: true
          path: /var/lib/openvswitch
        kind: generic
        services:
        - openvswitch
        - ovs-vswitchd
        - ovsdb-server
    runai:
      categories: []
      extras:
        training_operator: false
        training_operator_version: 1.9.2
      labels:
      - node-role.kubernetes.io/runai-system=true
      labelset:
        name: runai-control-plane
    skip_dns_configuration_check: false
    skip_image_update: true
    skip_install_repos: true
    skip_kube_version_check: false
    skip_netq_prerequisites_checks: false
    skip_package_manager_update_check: true
    skip_packages: false
    skip_reboot: false
    use_kata: false
    user:
      role: edit
    worker:
      categories:
      - k8s-cp-nodes
      configuration_overlay:
        name: kube-default-worker
        priority: 500
      labels:
      - node-role.kubernetes.io/worker
      nodes: []
      packages:
      - cm-kube-diagnose
      - nginx
      platform_specific_packages:
        redhat:
        - conntrack-tools
        - nginx-all-modules
        - python3-netifaces
        suse:
        - conntrack-tools
        - python3-netifaces
        ubuntu:
        - conntrack
        - python3-netifaces
        ubuntu-2404:
        - libnginx-mod-stream
